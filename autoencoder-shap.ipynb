{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\n\nclass AE(torch.nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.features = features\n        self.num_feat = features.shape[0]\n        self.check_vector = torch.zeros(self.num_feat)\n        self.losses = []\n        # Building an linear encoder with Linear\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(self.num_feat, 32),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(32, 16),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(16, 8)\n        )\n         \n        # Building an linear decoder with Linear\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(8, 16),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(16, 32),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(32, self.num_feat),\n            torch.nn.LeakyReLU(0.1))\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \n    def train(self, train_loader, epochs = 10, lr = 1e-3):\n        # Model Initialization\n \n        # Validation using MSE Loss function\n        loss_function = torch.nn.MSELoss()\n \n        # Using an Adam Optimizer with lr = 0.001\n        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = 1e-8)\n        outputs = []\n        \n        for epoch in range(epochs):\n            for (sample) in train_loader:\n                # Output of Autoencoder\n                reconstructed = self.forward(sample)\n                self.check_vector = (self.check_vector + torch.sum(sample, 0)/sample.size(0))/2\n                # Calculating the loss function\n                loss = loss_function(reconstructed, sample)\n        \n                # The gradients are set to zero,\n                # the gradient is computed and stored.\n                # .step() performs parameter update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n       \n                # Storing the losses in a list for plotting\n                self.losses.append(loss.detach().numpy())\n            outputs.append((epochs, sample, reconstructed))\n        for i in range(self.check_vector.size(dim=0)):\n            if self.check_vector[i] == 0:\n                self.check_vector[i] = 1\n            else:\n                self.check_vector[i] = 0\n        ###############\n        return self.losses \n    \n    def check_tensor(self, sample):\n        check = torch.mul(self.check_vector, sample)\n        combine_error = []\n        if torch.sum(check) == 0:\n            # nomal    \n            return False,1\n        else:\n            # abnomal\n            for i in range(self.num_feat):\n                if check[i] != 0:\n                    combine_error.append(self.features[i])\n            return True,combine_error\n\n    def test(self,test_tensor,mode):\n        loss_function = torch.nn.MSELoss()\n        losses = []\n        for sample in test_tensor:\n            loss = loss_function(self.forward(sample), sample)\n            losses.append(loss.detach().numpy())\n        threshold = np.mean(losses) + 3 * np.std(losses)\n        label = []\n        list_combine_error = []\n        ## Test code\n        for i in range(len(losses)):\n            check, combine_error = self.check_tensor(test_tensor[i])\n            if check:\n                label.append(2)\n                list_combine_error.append([i, combine_error])\n            else:\n                if losses[i] > threshold:\n                    label.append(1)\n                else: \n                    label.append(0)\n#         from sklearn.metrics import classification_report\n#         print(classification_report(np.zeros((pd.DataFrame(label)).shape[0]),label))\n        if mode == True:\n            return np.array(losses)\n        else:\n            return label, list_combine_error\n\ndef principal_combine(shap_value, percent):\n    shap_value = abs(shap_value)\n    sum_shap = np.sum(shap_value)\n    temp_shap = 0\n    shap_value_sort = sorted( shap_value, reverse = True)\n    shap_sort = np.argsort(-shap_values)\n    count_feat = 0\n    for i in range(len(shap_value)):\n        temp_shap = temp_shap + shap_value_sort[i]\n        if (temp_shap >= percent * sum_shap):\n            count_feat = i + 1\n            break\n    return shap_sort[:count_feat]\n\ndef main():\n    dataset = pd.DataFrame()\n    path_1 = \"/kaggle/input/data-17-02-2023-60s\"\n    dir_list_1 = os.listdir(path_1)\n    for name_file in dir_list_1:\n        dataset = pd.concat([dataset,pd.read_csv(path_1 + \"/\" + name_file)])\n    path_2 = \"/kaggle/input/test-data-21-02-2023\"\n    dir_list_2 = os.listdir(path_2)\n    for name_file in dir_list_2:\n        dataset = pd.concat([dataset,pd.read_csv(path_2 + \"/\" + name_file)])\n    dataset.fillna(0, inplace=True)\n    dataset = dataset.drop(['EVENT_TIME', 'Unnamed: 0'], axis = 1)\n    start = time.time()\n    \n    train, test = train_test_split(dataset, test_size=0.02,shuffle=False)\n    features = train.columns\n    scaler = MinMaxScaler()\n    scaler.fit(train)\n    train = scaler.transform(train)\n    test = scaler.transform(test)\n    train = pd.DataFrame(train, columns = features)\n    test = pd.DataFrame(test, columns = features)\n    train_tensor = torch.Tensor(np.array(train))\n    test_tensor = torch.Tensor(np.array(test))\n    train_loader = torch.utils.data.DataLoader(dataset = train_tensor,batch_size = 16,shuffle = True)\n    \n    start = time.time()\n    model = AE(features)\n    losses_train = model.train(train_loader, 10, 0.001)\n    print( \"Training time: \", time.time() - start, \"s\")\n    plt.style.use('bmh')\n    plt.xlabel('Sample')\n    plt.ylabel('Train testing')\n    plt.plot(losses_train)\n    \n    start = time.time()\n    losses_test = model.test(test_tensor, True)\n    print(\"Testing time: \", time.time() - start, \"s\")\n    plt.style.use('bmh')\n    plt.xlabel('Sample')\n    plt.ylabel('Loss testing')\n    plt.plot(losses_test)\n    \n    start = time.time()\n    # new combine\n    label, list_combine_error = model.test(test_tensor, False)\n    for com_err in list_combine_error:\n        print(\"Time error: \", com_err[0], \", Root cause: \", com_err[1])\n    # the cover function \n    def f(train):\n        train_tensor = torch.Tensor(np.array(train))\n        return model.test(train_tensor, True)\n    # shap modun\n    import shap\n    shap.initjs()\n    explainer = shap.KernelExplainer(f, train)\n    list_shap_values = []\n    position = []\n    for i in range(len(label)):\n        if label[i] == 1:\n            shap_values = explainer.shap_values(test.iloc[i,:], nsamples=500)\n            list_shap_values.append(shap_values)\n            position.append(i+1)\n#         shap.force_plot(explainer.expected_value, shap_values, test.iloc[i,:])  \n    for i in range(len(list_shap_values)):\n        feat = []\n        for j in principal_combine(list_shap_values[i], 0.95):\n            feat.append(features[j])\n        print(\"Time error: \", position[i], \", Root cause: \", feat)\n    shap.force_plot(explainer.expected_value, list_shap_values[0], test.iloc[134,:])\n    # shap.summary_plot(shap_values,test.iloc[134,:])\n    # shap.summary_plot(shap_values, test.iloc[134,:], plot_type='bar')\n    print(\"Detect root cause time: \", time.time() - start, \"s\")\n    \n    import matplotlib.pyplot as plt\n    from matplotlib import style\n    #Getting unique labels\n \n    u_labels = np.unique(np.array(label))\n \n    #plotting the results:\n    y = pd.DataFrame()\n    y['stt'] = range(185)\n    y['loss'] = losses\n    y['label'] = label\n    plt.style.use('bmh')\n    for i in u_labels:\n        plt.scatter(y[label == i].iloc[:,0] , y[label == i].iloc[:,1], label = i)\n    plt.xlabel('Sample')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"f7e03797-adde-4b8b-b4f3-fe38e12af5a8","_cell_guid":"e45a1380-ce50-45be-a631-969a5d47ab41","collapsed":false,"execution":{"iopub.status.busy":"2023-03-06T10:59:55.728648Z","iopub.execute_input":"2023-03-06T10:59:55.729133Z","iopub.status.idle":"2023-03-06T10:59:55.799305Z","shell.execute_reply.started":"2023-03-06T10:59:55.729034Z","shell.execute_reply":"2023-03-06T10:59:55.797198Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\nhttps://discuss.pytorch.org/t/autoencoder-testing-encoder-output/29988\nhttps://www.vielina.com/Uploads/Articles/Docs/downloads/nguyenthithanhnga/Tom%20tat%20LA..pdf","metadata":{"_uuid":"637a811f-732a-4a47-aa64-7c2459752d2d","_cell_guid":"90e373fc-f3e0-4302-a484-ac5e4a05bbb8","papermill":{"duration":0.004188,"end_time":"2023-01-11T09:04:08.404565","exception":false,"start_time":"2023-01-11T09:04:08.400377","status":"completed"},"tags":[],"trusted":true}},{"cell_type":"markdown","source":"https://visualstudiomagazine.com/Articles/2021/04/13/Autoencoder-Anomaly-Detection.aspx?Page=2\nhttps://learn.microsoft.com/en-us/windows/ai/windows-ml/what-is-a-machine-learning-model\nhttps://nttuan8.com/bai-1-tensor/","metadata":{"_uuid":"4c67fe8b-c264-4aaf-b40c-d43b570b3df6","_cell_guid":"d0d68740-a9dc-489c-b966-ce79dced37a2","papermill":{"duration":0.005637,"end_time":"2023-01-11T09:04:10.984497","exception":false,"start_time":"2023-01-11T09:04:10.978860","status":"completed"},"tags":[],"trusted":true}},{"cell_type":"code","source":"# import seaborn as sns\n# # Vẽ biểu đồ swarm\n# plt.figure(figsize=(16, 8))\n# sns.histplot(data = losses, bins=1000)\n# plt.xlabel('Scale', fontsize=16)\n# plt.ylabel('cm', fontsize=16)\n# plt.title(\"Histogram of Sepal.Width\", fontsize=18)","metadata":{"_uuid":"1d6155ef-643d-4405-bfda-1fc6bec17939","_cell_guid":"f4f8b973-9401-41aa-b867-564eba54f52a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://phamdinhkhanh.github.io/deepai-book/ch_appendix/appendix_matplotlib.html","metadata":{"_uuid":"aa586ee8-0d42-4e51-b800-1d286f7aff72","_cell_guid":"6e2baa88-993e-47a7-9cbe-3e8e98410019","trusted":true}}]}