{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom torch.autograd import Variable\nimport os\nimport time\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nclass Dataloader(object):\n    def __init__(self, data, seq_length=16, batch_size = 1):\n        self.data = pd.DataFrame(data)\n        self.seq_length = seq_length\n        self.batch_size = batch_size\n        self.flag = True\n        self.index_start = 0\n        \n    def get_item(self):\n        #print(self.data)\n        out_put = []\n        out_label = []\n        if (self.index_start + self.seq_length + 1 == self.data.shape[0]):\n            self.flag = False\n        for index in range(self.batch_size):\n            out_put.append(self.data.iloc[range(self.index_start + index, self.seq_length + self.index_start + index)])\n            out_label.append(self.data.iloc[self.seq_length + self.index_start + index,:])\n        \n        self.index_start = self.index_start + 1   \n        return torch.Tensor(np.array(out_put[:])), torch.Tensor(np.array(out_label))   \n#### AutoEncoder #####\nclass AE(torch.nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        # Building an linear encoder with Linear\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(input_size, input_size//2),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(input_size//2, input_size//4),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(input_size//4, input_size//8)\n        )\n         \n        # Building an linear decoder with Linear\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(input_size//8, input_size//4),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(input_size//4, input_size//2),\n            torch.nn.LeakyReLU(0.1),\n            torch.nn.Linear(input_size//2, input_size),\n            torch.nn.LeakyReLU(0.1))\n#### LSTM ####\nclass LSTM(nn.Module):\n\n    def __init__(self, num_feature, seq_length, hidden_size=64, num_layers=15):\n        super(LSTM, self).__init__()\n        \n        self.num_layers = num_layers\n        self.num_feature = int(num_feature)\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        \n        self.lstm = nn.LSTM(input_size= self.num_feature, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, self.num_feature)\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n        #h_0 = Variable(torch.zeros(x.size(0), self.hidden_size))\n        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n        #c_0 = Variable(torch.zeros(x.size(0), self.hidden_size))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc(h_out)\n        \n        return out\n#### AE-LSTM ####\nclass AutoEncoder_LSTM(nn.Module):\n    def __init__(self, input_size, seq_length):\n        super().__init__()\n        self.input_size = input_size\n        self.seq_length = seq_length\n        self.autoencoder = AE(input_size)\n        self.lstm = LSTM(input_size/8, seq_length = seq_length)\n \n    def forward(self, x):\n        data_encoder = self.autoencoder.encoder(x)\n        data_lstm = self.lstm(data_encoder)\n        output = self.autoencoder.decoder(data_lstm)\n        return output\n    ##### train method #####\n    def train(self, data, epochs, learning_rate, batch_size):\n        losses = []\n        optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = 1e-8)\n        loss_function = torch.nn.MSELoss()\n        for epoch in range(epochs):\n            dataloader = Dataloader(data, seq_length = self.seq_length)\n            while(dataloader.flag == True):\n                data_window, data_label = dataloader.get_item()\n                predict = self.forward(data_window)\n                loss = loss_function(predict, data_label)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                losses.append(loss.detach().numpy())\n        return losses\n    ##### test method #####\n    def test(self, test):\n        losses = []\n        for i in range(self.seq_length):\n            losses.append(0)\n        loss_function = torch.nn.MSELoss()\n        dataloader = Dataloader(test, seq_length = self.seq_length)\n        while(dataloader.flag == True):\n            data_window, data_label = dataloader.get_item()\n            loss = loss_function(self.forward(data_window), data_label)\n            losses.append(loss.detach().numpy())\n        threshold = np.mean(losses) + 3 * np.std(losses) \n        label = []\n        for i in range(len(losses)):\n#             if self.check_tensor(test_tensor[i]):\n#                 label.append(2)\n#             else:\n#                 if losses[i] > threshold:\n#                     label.append(1)\n#                 else: \n#                     label.append(0)\n            if losses[i] > threshold:\n                label.append(1)\n            else: \n                label.append(0)\n        return losses, label\ndef main():\n    dataset = pd.DataFrame()\n    path_1 = \"/kaggle/input/data-17-02-2023-60s\"\n    dir_list_1 = os.listdir(path_1)\n    for name_file in dir_list_1:\n        dataset = pd.concat([dataset,pd.read_csv(path_1 + \"/\" + name_file)])\n    path_2 = \"/kaggle/input/test-data-21-02-2023\"\n    dir_list_2 = os.listdir(path_2)\n    for name_file in dir_list_2:\n        dataset = pd.concat([dataset,pd.read_csv(path_2 + \"/\" + name_file)])\n    dataset.fillna(0, inplace=True)\n    dataset = dataset.drop(['EVENT_TIME', 'Unnamed: 0'], axis = 1)\n    start = time.time()\n    \n    train, test = train_test_split(dataset, test_size=0.02,shuffle=False)\n    features = train.columns\n    scaler = MinMaxScaler()\n    scaler.fit(train)\n    train = scaler.transform(train)\n    test = scaler.transform(test)\n    train = pd.DataFrame(train, columns = features)\n    test = pd.DataFrame(test, columns = features)\n    \n    start = time.time()\n    AE_LSTM = AutoEncoder_LSTM(seq_length = 10,input_size = train.shape[1])\n    losses_train = AE_LSTM.train(train, epochs = 3,learning_rate = 0.0001, batch_size = 1)\n    plt.style.use('bmh')\n    plt.xlabel('Sample')\n    plt.ylabel('Loss training')\n    plt.plot(losses_train)\n    print( \"Training time: \", time.time() - start, \"s\")\n    \n    start = time.time()\n    losses_test, label = AE_LSTM.test(test)\n    print(\"Testing time: \", time.time() - start, \"s\")\n    plt.style.use('bmh')\n    plt.xlabel('Sample')\n    plt.ylabel('Loss testing')\n    plt.plot(losses_test)\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-06T09:29:20.073011Z","iopub.execute_input":"2023-03-06T09:29:20.073872Z","iopub.status.idle":"2023-03-06T09:29:20.134465Z","shell.execute_reply.started":"2023-03-06T09:29:20.073762Z","shell.execute_reply":"2023-03-06T09:29:20.133041Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_27/1356228009.py\"\u001b[0;36m, line \u001b[0;32m179\u001b[0m\n\u001b[0;31m    a b c\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1356228009.py, line 179)","output_type":"error"}]}]}