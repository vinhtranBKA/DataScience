{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Link: \nhttps://towardsdatascience.com/pytorch-lstms-for-time-series-data-cd16190929d7\nhttps://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb#scrollTo=a2vvHeFgVtCp","metadata":{"_uuid":"7496fb9f-f619-4fd0-8927-efb8b3930f54","_cell_guid":"b6f45534-8384-4b83-891d-b9fdf0e3bf3a","trusted":true}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport time\n\nclass Dataloader(object):\n    def __init__(self, data, seq_length = 16, batch_size = 16):\n        self.data = pd.DataFrame(data)\n        self.seq_length = seq_length\n        self.batch_size = batch_size\n        self.flag = True\n        self.index_start = 0\n        \n    def get_item(self):\n        out_put = []\n        out_label = []\n        if (self.index_start + self.seq_length + 2 * self.batch_size - 1 >= self.data.shape[0]):\n            self.flag = False\n#         print(self.index_start)\n        for index in range(self.batch_size):\n            out_put.append(self.data.iloc[range(self.index_start + index, self.seq_length + self.index_start + index)])\n            out_label.append(self.data.iloc[self.seq_length + self.index_start + index,:])\n        \n        self.index_start = self.index_start + self.batch_size   \n        return torch.Tensor(np.array(out_put[:])), torch.Tensor(np.array(out_label))\nclass LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, batch_size):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        self.batch_size = batch_size\n        \n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n        \n        c_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc(h_out)\n        \n        return out\n    #####***********##########\n    def train(self, data, epochs, learning_rate):\n        losses = []\n        optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = 1e-8)\n        loss_function = torch.nn.MSELoss()\n        for epoch in range(epochs):\n            print(epoch)\n            dataloader = Dataloader(data, seq_length = self.seq_length,batch_size= self.batch_size)\n            while(dataloader.flag == True):\n                data_window, data_label = dataloader.get_item()\n                predict = self.forward(data_window)\n                loss = loss_function(predict, data_label)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                losses.append(loss.detach().numpy())\n        return losses\n    def test(self, test):\n        losses = []\n        for i in range(self.seq_length):\n            losses.append(0)\n        loss_function = torch.nn.MSELoss()\n        dataloader = Dataloader(test, seq_length = self.seq_length, batch_size = self.batch_size)\n        while(dataloader.flag == True):\n            data_window, data_label = dataloader.get_item()\n            loss = loss_function(self.forward(data_window), data_label)\n            losses.append(loss.detach().numpy())\n        threshold = np.mean(losses) + 3 * np.std(losses) \n        label = []\n        for i in range(len(losses)):\n            if losses[i] > threshold:\n                label.append(1)\n            else: \n                label.append(0)\n        return losses, label\n\ndataset = pd.DataFrame()\npath_1 = \"/kaggle/input/data-17-02-2023-60s\"\ndir_list_1 = os.listdir(path_1)\nfor name_file in dir_list_1:\n    dataset = pd.concat([dataset,pd.read_csv(path_1 + \"/\" + name_file)])\n    \npath_2 = \"/kaggle/input/test-data-21-02-2023\"\ndir_list_2 = os.listdir(path_2)\nfor name_file in dir_list_2:\n    dataset = pd.concat([dataset,pd.read_csv(path_2 + \"/\" + name_file)])\n    \ndataset.fillna(0, inplace=True)\ndataset = dataset.drop('EVENT_TIME', axis = 1)\ndataset = dataset.drop('Unnamed: 0', axis = 1)\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataset, test_size=0.02,shuffle=False)\nfeatures = train.columns\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\ntrain = pd.DataFrame(train, columns = features)\ntest = pd.DataFrame(test, columns = features)\nimport time\na = time.time()\nmodel = LSTM(num_classes = train.shape[1], input_size =train.shape[1], hidden_size = 64, num_layers = 1, seq_length = 10, batch_size = 1)\nlosses_train = model.train(train, epochs = 3,learning_rate = 0.0001)\nprint(time.time() - a)\na = time.time()\nlosses_test, label = model.test(test)\nprint(time.time() - a)\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nplt.xlabel('Sample')\nplt.ylabel('Loss')\nplt.plot(losses_train)","metadata":{"_uuid":"672cb13c-68a4-4160-b5f1-8736dabf3740","_cell_guid":"05fb1935-be25-4341-b747-845070d96a48","collapsed":false,"execution":{"iopub.status.busy":"2023-03-03T09:50:49.237127Z","iopub.execute_input":"2023-03-03T09:50:49.237893Z","iopub.status.idle":"2023-03-03T09:50:49.602709Z","shell.execute_reply.started":"2023-03-03T09:50:49.237806Z","shell.execute_reply":"2023-03-03T09:50:49.601210Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}