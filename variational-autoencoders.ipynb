{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":19.59469,"end_time":"2023-01-06T02:13:38.853144","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-06T02:13:19.258454","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Data Argument","metadata":{"papermill":{"duration":0.004521,"end_time":"2023-01-06T02:13:29.356880","exception":false,"start_time":"2023-01-06T02:13:29.352359","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport time\n\nclass VariationalEncoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(VariationalEncoder, self).__init__()\n        self.linear1 = nn.Linear(63, 32)\n        self.linear2 = nn.Linear(32, latent_dims)\n        self.linear3 = nn.Linear(32, latent_dims)\n\n        self.N = torch.distributions.Normal(0, 1)\n        self.N.loc = self.N.loc#.cuda() # hack to get sampling on the GPU\n        self.N.scale = self.N.scale#.cuda()\n        self.kl = 0\n\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        mu =  self.linear2(x)\n        sigma = torch.exp(self.linear3(x))\n        z = mu + sigma*self.N.sample(mu.shape)\n        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n        return z\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(Decoder, self).__init__()\n        self.linear1 = nn.Linear(latent_dims, 32)\n        self.linear2 = nn.Linear(32, 63)\n\n    def forward(self, z):\n        z = F.relu(self.linear1(z))\n        z = self.linear2(z)\n        return z\n    \nclass VariationalAutoencoder(nn.Module):\n    def __init__(self, latent_dims):\n        super(VariationalAutoencoder, self).__init__()\n        self.encoder = VariationalEncoder(latent_dims)\n        self.decoder = Decoder(latent_dims)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        return self.decoder(z)\n    def train(self, data, lr, epochs=200):\n        opt = torch.optim.Adam(self.parameters(), lr = lr)\n        loss_vae = []\n        for epoch in range(epochs):\n            for x in data:\n                for sample in x:\n                    sample = sample.to(device) # GPU\n                    opt.zero_grad()\n                    sample_hat = self.forward(sample)\n                    loss = ((sample - sample_hat)**2).sum() + self.encoder.kl\n                    loss.backward()\n                    opt.step()\n                    loss_vae.append(loss.item())\n                #print( 'loss epoch %d: %f'% (epoch, loss.item()))\n        return loss_vae\n    def test(self, data):\n        loss_vae = []\n        for sample in data:\n            sample_hat = self.forward(sample)\n            loss = ((sample - sample_hat)**2).sum() + self.encoder.kl\n            loss_vae.append(loss.item())\n        return loss_vae\n    \ndataset = pd.DataFrame()\npath_1 = \"/kaggle/input/data-17-02-2023-60s\"\ndir_list_1 = os.listdir(path_1)\nfor name_file in dir_list_1:\n    dataset = pd.concat([dataset,pd.read_csv(path_1 + \"/\" + name_file)])\n    \npath_2 = \"/kaggle/input/test-data-21-02-2023\"\ndir_list_2 = os.listdir(path_2)\nfor name_file in dir_list_2:\n    dataset = pd.concat([dataset,pd.read_csv(path_2 + \"/\" + name_file)])\n    \ndataset.fillna(0, inplace=True)\ndataset = dataset.drop('EVENT_TIME', axis = 1)\ndataset = dataset.drop('Unnamed: 0', axis = 1)\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataset, test_size=0.02, shuffle=False)\nfeatures = train.columns\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\ntrain = pd.DataFrame(train, columns = features)\ntest = pd.DataFrame(test, columns = features)\ntorch.manual_seed(111)\nbatch_size = 32\ntrain_loader = torch.utils.data.DataLoader(torch.Tensor(np.array(train)), batch_size=batch_size, shuffle=True)\ndevice = \"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nimport time\nstart = time.time()\nlatent_dims = 16\nvae = VariationalAutoencoder(latent_dims).to(device) # GPU\nloss_train = vae.train(train_loader, 0.001, 10)\n\npd.DataFrame(loss_train).plot()\nstart = time.time()\nloss_test = vae.test(torch.Tensor(np.array(test)))\npd.DataFrame(loss_test).plot()\nend = time.time()\nprint(end - start)","metadata":{"papermill":{"duration":3.039271,"end_time":"2023-01-06T02:13:32.399869","exception":false,"start_time":"2023-01-06T02:13:29.360598","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-03T09:39:15.853229Z","iopub.execute_input":"2023-03-03T09:39:15.855047Z","iopub.status.idle":"2023-03-03T09:39:19.687708Z","shell.execute_reply.started":"2023-03-03T09:39:15.854919Z","shell.execute_reply":"2023-03-03T09:39:19.686108Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Link: \nhttps://avandekleut.github.io/vae/","metadata":{"papermill":{"duration":0.003696,"end_time":"2023-01-06T02:13:32.407904","exception":false,"start_time":"2023-01-06T02:13:32.404208","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Evaluation model","metadata":{"papermill":{"duration":0.003923,"end_time":"2023-01-06T02:13:37.489232","exception":false,"start_time":"2023-01-06T02:13:37.485309","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('bmh')\nplt.xlabel('Sample')\nplt.ylabel('Loss')\nplt.plot(loss_train)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T09:39:20.258788Z","iopub.status.idle":"2023-03-03T09:39:20.259379Z","shell.execute_reply.started":"2023-03-03T09:39:20.259073Z","shell.execute_reply":"2023-03-03T09:39:20.259093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('bmh')\nplt.xlabel('Sample')\nplt.ylabel('Loss')\nplt.plot(loss_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-03T09:39:20.260431Z","iopub.status.idle":"2023-03-03T09:39:20.260820Z","shell.execute_reply.started":"2023-03-03T09:39:20.260639Z","shell.execute_reply":"2023-03-03T09:39:20.260656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://curiousily.com/posts/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python/","metadata":{}}]}