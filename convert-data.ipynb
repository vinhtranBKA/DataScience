{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":110.077853,"end_time":"2023-01-08T03:15:15.177647","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-08T03:13:25.099794","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html\nhttps://stackoverflow.com/questions/53020609/the-axis-argument-to-unique-is-not-supported-for-dtype-object","metadata":{}},{"cell_type":"markdown","source":"# Convert data test","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport os\n\ndef pre_process(data):\n    feature = ['EVENT_ID','L_CAUSE_PROT_TYPE','CAUSE_CODE','EVENT_TIME']\n    pre_data = data[feature]\n    pre_data.fillna('0', inplace=True)\n    pre_data = pre_data.replace(['unspecified', 'unknown-E-RAB-ID'], -1)\n    pre_data['CAUSE_CODE'] = pre_data['CAUSE_CODE'].astype('int')\n    return pre_data\n\nclass slicing_window(object):\n    def __init__(self, data, feature_time, window_size):\n        self.data = data\n        self.window_size = window_size\n        self.feature_time = np.array(data[feature_time]) \n        self.temp_time = math.ceil(self.feature_time[0]/self.window_size) * self.window_size\n        self.flag = True\n    \n    def get_time(self):\n        return self.temp_time\n    \n    def get_item(self):\n        temp_time = self.temp_time\n        if (self.temp_time > (self.feature_time[-1] - 2 * self.window_size)):\n            self.flag = False\n        self.temp_time = self.temp_time + self.window_size\n        return (self.data).loc[((self.data)['EVENT_TIME'] >= temp_time) & ((self.data)['EVENT_TIME'] < self.temp_time)]\n\ndef KPI_eventID_ccode(data, eventID, cause_code):\n    count = 0\n    list_eventID = list(data['EVENT_ID'])\n    list_cause_code = list(data['CAUSE_CODE'])\n    for i in range(data.shape[0]):\n        if ((list_eventID[i] == eventID) and (list_cause_code[i] == cause_code)):\n            count = count + 1\n    if data.shape[0] == 0:\n        return 0\n    else:\n        return count/(data.shape[0])\n\n\ndef make_new_data(data, window_size):\n    loader_time = slicing_window(data,'EVENT_TIME',window_size * 1000)\n    new_data =  pd.DataFrame()\n    data.sort_values(by=['EVENT_TIME'])\n    while(loader_time.flag == True):\n        temp_time = loader_time.get_time()\n        data_window = loader_time.get_item()\n        unique_combine, count = np.unique(data_window.drop(['EVENT_TIME'], axis = 1).to_numpy().astype(\"<U22\"), axis=0, return_counts=True)\n        name_feature = []\n        kpi_combine = count/(count.sum())\n        kpi_combine = np.insert(kpi_combine, len(kpi_combine), temp_time, axis=0)\n        for pre_name in unique_combine:\n            name = pre_name[0] + \"_\" + pre_name[1] + \"_\"  + pre_name[2]\n            name_feature.append(name)\n        name_feature.append('EVENT_TIME')\n        new_data_unit = pd.DataFrame(np.array([kpi_combine]), columns = name_feature)\n        new_data = pd.concat([new_data, new_data_unit])\n    new_data.fillna(0, inplace=True)\n    return new_data\n    \ndef main(path, window_size):\n    data = pd.read_csv(path, sep=\";\", header=None)\n    data.columns = ['EVENT_ID', 'EVENT_RESULT', 'DURATION', 'REQUEST_RETRIES', \n                    'SUB_TYPE', 'MSISDN', 'IMSI','MTMSI','IMEISV','MMEGI','MMEC',\n                    'TAC','ECI','SGW','SGSN','L_CAUSE_PROT_TYPE','CAUSE_CODE',\n                    'SUB_CAUSE_CODE','APN','PDN_DEFAULT_BEARER_ID','PDN_PAA',\n                    'PDN_PGW','ORIGINATING_CAUSE_PROT_TYPE','ORIGINATING_CAUSE_CODE',\n                    'CSG_ID','OLD_MTMSI','OLD_TAC','OLD_MMEGI','OLD_MMEC','OLD_ECI',\n                    'OLD_SGW','OLD_SGSN','MSC','TARGET_LAC','LAC','RAC','CI',\n                    'HANDOVER_NODE_ROLE','HANDOVER_RAT_CHANGE_TYPE','HANDOVER_SGW_CHANGE_TYPE',\n                    'TARGET_RNC_ID','TARGET_MACRO_ENODEB_ID','SRVCC_TYPE',\n                    'CS_FALLBACK_SERVICE_TYPE','CSFB_TRIGGERED','L_SERVICE_REQ_TRIGGER','COMBINED_TAU_TYPE',\n                    'DETACH_TRIGGER','EVENT_TIME','PAGING_ATTEMPTS','UE_REQUESTED_APN','DATE_HOUR']\n    pre_data = pre_process(data)\n    new_data = make_new_data(pre_data,window_size)\n    return new_data\n\n\nif __name__ == \"__main__\":\n    path = \"/kaggle/input/data-full-10022023\"\n    dir_list = os.listdir(path)\n    new_data = pd.DataFrame()\n    for name_file in dir_list:\n        new_data = pd.concat([new_data,main('/kaggle/input/data-full-10022023/' + name_file, 60)])\n        new_data.fillna(0, inplace=True)\n    new_data.to_csv(\"New_data_10022023_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-10T04:23:33.686219Z","iopub.execute_input":"2023-02-10T04:23:33.686793Z","iopub.status.idle":"2023-02-10T04:23:33.693262Z","shell.execute_reply.started":"2023-02-10T04:23:33.686757Z","shell.execute_reply":"2023-02-10T04:23:33.692331Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"markdown","source":"# Convert data train","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport os\n\ndef pre_process(data):\n    feature = ['EVENT_ID','L_CAUSE_PROT_TYPE','CAUSE_CODE','EVENT_TIME']\n    pre_data = data[feature]\n    pre_data.fillna('0', inplace=True)\n    pre_data = pre_data.replace(['unspecified', 'unknown-E-RAB-ID'], -1)\n    pre_data['CAUSE_CODE'] = pre_data['CAUSE_CODE'].astype('int')\n    return pre_data\n\nclass slicing_window(object):\n    def __init__(self, data, feature_time, window_size = 1000, repeat_size=0.1):\n        self.data = data\n        self.window_size = window_size\n        self.feature_time = np.array(data[feature_time]) \n        self.temp_time = math.ceil(self.feature_time[0]/self.window_size) * self.window_size\n        self.flag = True\n        self.repeat_size = repeat_size\n    \n    def get_time(self):\n        return self.temp_time\n    \n    def get_item(self):\n        temp_time = self.temp_time\n        if (self.temp_time > (self.feature_time[-1] - (1 + self.repeat_size) * self.window_size)):\n            self.flag = False\n        self.temp_time = int(self.temp_time + self.window_size * self.repeat_size)\n        return (self.data).loc[((self.data)['EVENT_TIME'] >= temp_time) & ((self.data)['EVENT_TIME'] < (temp_time + self.window_size))]\n\ndef KPI_eventID_ccode(data, eventID, cause_code):\n    count = 0\n    list_eventID = list(data['EVENT_ID'])\n    list_cause_code = list(data['CAUSE_CODE'])\n    for i in range(data.shape[0]):\n        if ((list_eventID[i] == eventID) and (list_cause_code[i] == cause_code)):\n            count = count + 1\n    if data.shape[0] == 0:\n        return 0\n    else:\n        return count/(data.shape[0])\n\n\ndef make_new_data(data, window_size, repeat_size):\n    loader_time = slicing_window(data,'EVENT_TIME',window_size * 1000, repeat_size)\n    new_data =  pd.DataFrame()\n    data.sort_values(by=['EVENT_TIME'])\n    while(loader_time.flag == True):\n        temp_time = loader_time.get_time()\n        data_window = loader_time.get_item()\n        unique_combine, count = np.unique(data_window.drop(['EVENT_TIME'], axis = 1).to_numpy().astype(\"<U22\"), axis=0, return_counts=True)\n        name_feature = []\n        kpi_combine = count/(count.sum())\n        kpi_combine = np.insert(kpi_combine, len(kpi_combine), temp_time, axis=0)\n        for pre_name in unique_combine:\n            name = pre_name[0] + \"_\" + pre_name[1] + \"_\"  + pre_name[2]\n            name_feature.append(name)\n        name_feature.append('EVENT_TIME')\n        new_data_unit = pd.DataFrame(np.array([kpi_combine]), columns = name_feature)\n        new_data = pd.concat([new_data, new_data_unit])\n    new_data.fillna(0, inplace=True)\n    return new_data\n    \ndef main(path, window_size, repeat_size):\n    data = pd.read_csv(path, sep=\";\", header=None)\n    data.columns = ['EVENT_ID', 'EVENT_RESULT', 'DURATION', 'REQUEST_RETRIES', \n                    'SUB_TYPE', 'MSISDN', 'IMSI','MTMSI','IMEISV','MMEGI','MMEC',\n                    'TAC','ECI','SGW','SGSN','L_CAUSE_PROT_TYPE','CAUSE_CODE',\n                    'SUB_CAUSE_CODE','APN','PDN_DEFAULT_BEARER_ID','PDN_PAA',\n                    'PDN_PGW','ORIGINATING_CAUSE_PROT_TYPE','ORIGINATING_CAUSE_CODE',\n                    'CSG_ID','OLD_MTMSI','OLD_TAC','OLD_MMEGI','OLD_MMEC','OLD_ECI',\n                    'OLD_SGW','OLD_SGSN','MSC','TARGET_LAC','LAC','RAC','CI',\n                    'HANDOVER_NODE_ROLE','HANDOVER_RAT_CHANGE_TYPE','HANDOVER_SGW_CHANGE_TYPE',\n                    'TARGET_RNC_ID','TARGET_MACRO_ENODEB_ID','SRVCC_TYPE',\n                    'CS_FALLBACK_SERVICE_TYPE','CSFB_TRIGGERED','L_SERVICE_REQ_TRIGGER','COMBINED_TAU_TYPE',\n                    'DETACH_TRIGGER','EVENT_TIME','PAGING_ATTEMPTS','UE_REQUESTED_APN','DATE_HOUR']\n    pre_data = pre_process(data)\n    new_data = make_new_data(pre_data,window_size, repeat_size)\n    return new_data\n\n\nif __name__ == \"__main__\":\n    path = \"/kaggle/input/data-full-10022023\"\n    dir_list = os.listdir(path)\n    new_data = pd.DataFrame()\n    for name_file in dir_list:\n        new_data = pd.concat([new_data,main('/kaggle/input/data-full-10022023/' + name_file, 30, 0.1)])\n        new_data.fillna(0, inplace=True)\n    new_data.to_csv(\"New_data_10022023_train.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}